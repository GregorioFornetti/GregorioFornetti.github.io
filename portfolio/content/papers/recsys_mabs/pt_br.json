{
    "title": "Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation",
    "img_alt": "Exploração no lado esquerdo com um restaurante bem conhecido e exploração no lado direito com pontos de interrogação",
    "abstract": "Os algoritmos de Multi-Armed Bandit (MAB) são amplamente utilizados em sistemas de recomendação que requerem aprendizado contínuo e incremental. Um aspecto central dos MABs é o equilíbrio entre exploração e aprofundamento, que consiste em escolher entre explorar itens com alta probabilidade de serem consumidos pelo usuário e explorar novos itens para coletar novas informações. Em *bandits* lineares contextuais, esse equilíbrio é particularmente importante, visto que muitas variantes compartilham a mesma estrutura de regressão linear e diferem principalmente em suas estratégias de exploração. Apesar de seu uso prevalente, a avaliação *offline* de MABs é cada vez mais reconhecida por suas limitações na avaliação confiável do comportamento de exploração. Este estudo realiza uma extensa comparação empírica *offline* de diversos MABs lineares. Surpreendentemente, em mais de 90% das bases de dados, um modelo linear guloso — sem qualquer tipo de exploração — consistentemente alcança os melhores resultados, muitas vezes superando ou igualando suas outras versões com estratégias de exploração. Essa observação é corroborada pela otimização de hiperparâmetros, que consistentemente favorece configurações que minimizam a exploração, sugerindo que o aprofundamento puro é a estratégia dominante nesses cenários de avaliação. Nossos resultados expõem inadequações significativas nos protocolos de avaliação *offline* para sistemas de recomendação, particularmente em relação à sua capacidade de refletir a verdadeira eficácia exploratória. Consequentemente, esta pesquisa ressalta a necessidade urgente de desenvolver metodologias de avaliação mais robustas, orientando futuras investigações sobre estruturas de avaliação alternativas para aprendizado interativo em sistemas de recomendação. O código-fonte de nossos experimentos está disponível publicamente em https://github.com/UFSCar-LaSID/exploit-over-explore.",
    "contributions_text": "Neste artigo, fui o principal responsável pelo código, elaborando sua estrutura e documentação (disponível no [GitHub](https://github.com/UFSCar-LaSID/exploit-over-explore)). Implementei o protocolo experimental incremental e os algoritmos MAB otimizados, que posteriormente levaram ao desenvolvimento da [biblioteca GOBRec](https://github.com/UFSCar-LaSID/gobrec). Também auxiliei no processo de revisão do artigo."
}